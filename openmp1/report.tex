\documentclass[a4paper]{article}

\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{listings}
\usepackage{color}

\definecolor{light-gray}{gray}{0.95}
\definecolor{dark-green}{rgb}{0, 0.5, 0}
\definecolor{dark-gray}{gray}{0.4}
\definecolor{Gray}{gray}{0.95}

\lstset{ %
	language = C,                   % choose the language of the code
	basicstyle = \small\ttfamily,   % the size and fonts that are used
	frame = single,                 % adds a frame around the code
	tabsize = 3,                    % sets default tabsize
	breaklines = true,              % sets automatic line breaking
	numbers = left,                 % where to put the line-numbers
	numberstyle = \footnotesize,    % the style of the line-numbers
	backgroundcolor = \color{Gray}, % the background color of the listing
	showstringspaces=false,
	keywordstyle=\color{blue},
}

\title{Assignment 1}
\author{Jacco Spoelder (s1348493) \and Xeryus Stokkel (s2332795)}

\begin{document}

\maketitle

\section{Exercise 1}
\begin{enumerate}[(a)]
	\item The "Hello world!" lines are interleaved because the statements in the first block are executed in parallel. This means that both threads will simultaneously execute the \texttt{printf("Hello ");} statement so you see that appearing twice. They will then execute the \texttt{printf("world!\textbackslash n");} statements in parallel which means that this is printed twice in a row. Because it happens in parallel the lines will be interleaved. We were not able to reproduce this interleaved effect on our own systems.
	
	The line "Have a nice day!" is not interleaved because only a single printf statement is executed in parallel, this simply comes down to executing this statement twice.
	\item The output is the following:
		\begin{lstlisting}
Hello world!
Have a nice day!
Hello world!
Have a nice day!
Have fun!
		\end{lstlisting}
		As we can see, everything in the first parallel block is executed twice. However the second pragma statement has no effect, the line "Have a nice day!" is only printed twice just like the other printf statement in the first block. This means that OpenMP doesn't support nesting or that it needs to be enabled explicitly.
	\item The output when the environment variable \texttt{OMP\_NESTING} is enabled is:
		\begin{lstlisting}
Hello world!
Hello world!
Have a nice day!
Have a nice day!
Have a nice day!
Have a nice day!
Have fun!
		\end{lstlisting}
		As we can see the line "Have a nice day!" is printed four times. We have 2 threads executing the main parallel block, so that means printing the line twice. But we also have a nested pragma that wants to execute the print statement in parallel. This means that the statement is executed an additional 2 times per already existing thread. So in total we get $2 \times 2 = 4$ total outputs. In general we can say that with N threads that the entire block will be executed N times and the nested pragma will be executed N times for each already existing block, so in general the amount of printed "Have a nice day!" statements will be $N \times N = N^2$.
\end{enumerate}

\section{Exercise 2}
\begin{enumerate}[(a)]
	\item We predict that the output of the program will look like below. \texttt{undefined} means that we do not know the output as the variable is not initialised before printing.
		\begin{lstlisting}
#i=31415
i=undefined
i=undefined
i=undefined
i=undefined
#i=31415
i=31415
i=31415
i=31415
i=31415
#i=31415
i=31415
i=31416
i=31417
i=31418
#i=31419
		\end{lstlisting}
		The compiler warns us about using \texttt{i} uninitialised in the first parallel block. The output was:
		\begin{lstlisting}
#i=31415
 i=0
 i=32734
 i=0
 i=0
#i=31415
 i=31415
 i=31415
 i=31415
 i=31415
#i=31415
 i=31415
 i=31417
 i=31416
 i=31418
#i=31419
		\end{lstlisting}
		As we can see our prediction was correct, the value of the first parallel section was indeed undefined: 0 and 32734 both occur without an indication of why because \texttt{i} was undefined. We also see that when the variable is shared that the value is affected across threads and that it also affects the value of the variable in the main thread.
	\setcounter{enumi}{0}
	\item When executing sequential the program will simply do the following $N$ times:
		\begin{enumerate}[1.]
			\item Set the value of $x$ to 1.
			\item Increment $x$ to 2.
			\item Multiply $x$ by 3 to obtain 6.
			\item Increment the element with index 6 in the histogram by one.
		\end{enumerate}
		This means that the resulting histogram will be \texttt{\{0 0 0 0 0 0 1000000 0 0 0\}}.
		
		After executing the program this has been shown to indeed be the case.
	\item The possibilities of what happens with 2 threads are the following:
		\begin{itemize}
			\item The same as with one thread.
			\item The same as with one thread but step 2 and 3 reversed, so $x = 1 \cdot 3 + 1 = 4$.
			\item Step 2 is skipped so the result is $x = 3$.
			\item Step 3 is skipped so the result is $x = 2$.
		\end{itemize}
		So the possibilities for the value of $x$ are 2, 3, 4 and 6.
	\item With \texttt{OMP\_NUM\_THREADS} = 2 the output is not consistent but the order of magnitudes of the numbers is always the same:
	\begin{lstlisting}
0: 0
1: 0
2: 62
3: 1363
4: 16
5: 0
6: 998559
7: 0
8: 0
9: 0
	\end{lstlisting}
	As we can see our prediction of the values of $x$ was correct. We can also see that most of the time the code executes correctly. The largest error occurs where the second section overwrites the first section. This is most likely because the calculations are done on a temporary variable $y$. Both sections will retrieve the value of $x$ and store them in $y$ and then do their calculations on this variable. Section one will then write the value of $y$ to $x$. Section 2 will then also do the same, but because it used the original value of $x$ it will overwrite the results of the first section.
	
	Similar events occur for the case where $x = 2$. For the case where $x=4$ the sections do not overwrite each other but section 2 finishes before section 1 starts and thus the calculation is done the wrong way around.
	
	For 24 threads the following happens (note that we ran this on a laptop with only 4 cores available):
	\begin{lstlisting}
0: 0
1: 0
2: 186
3: 565
4: 183
5: 0
6: 999066
7: 0
8: 0
9: 0
	\end{lstlisting}
	The results are very similar. We think this is because only two threads actually do any work and 22 are sitting around and idling. This means that the results are the same for 2 threads. It is very likely that the program takes a lot longer because of the overhead of creating and destroying 24 threads for every iteration of the for loop.
	\item The results for this program are shown in \autoref{tbl:shared}. It clearly shows that the execution time for the reductive part goes down when the number of threads goes up. For the shared part the execution time increases quite dramatically!
	
	Reduction executes the code in parallel and adds the results together after all the threads finish. The shared variable code updates the results in every iteration, so all the threads compete for the same variable which results in large waiting times. Reduction can increase its speed because there is only a slowdown at the end where all the results are added together. The overhead of adding the variables together is also only minimal, with 8 threads only 8 sums need to be added together so the overhead is minimal compared to total number of calculations (100 million).
	
	Shared code needs to wait these 100 million calculations to update. Whenever a thread updates the other threads need to wait for this process to be completed. This means that with 4 threads that there can be a queue of up to 3 threads idling because they need to wait for the shared variable to be released. With 8 threads the queue increases to a maximum of 7, this means that a lot more idling is done and no calculations are done in that time. So the program is spending most of the time waiting to update instead of doing the calculations. This is especially noticeable with this program where the calculation is really simple so updating the value in memory takes a relatively long time.
	
	With the step from 1 to 2 threads we can see that the calculation time quadruples while for the other steps it only doubles. From this we can say that the calculation takes less time than updating the shared variable because with 1 thread there is no wait time while with 2 threads there is a lot of wait time. This means that each thread spends most of its time waiting on the other thread instead of doing the calculation. With 4 and 8 threads the time only doubles so we can say that the wait time also doubles. This means that the queue to update the shared variable is always full because a thread will do the calculation before any other thread has been able to update the shared variable.
\end{enumerate}

\begin{table}
	\centering
	\caption{Timings of the reduction and the shared part of the program}
	\label{tbl:shared}
	\begin{tabular}{r|l|l}
		Threads & Reduction & Shared \\ \hline
		1 & 0.389616 & 1.895604 \\
		2 & 0.195814 & 16.035956 \\
		4 & 0.149522 & 35.542965 \\
		8 & 0.102292 & 72.173635
	\end{tabular}
\end{table}

\section{Computing $\pi$}
\begin{enumerate}[(a)]
	\item The function which computes each chunk of $N$ is suited for parallelization. It is a for-loop, and each time the same variable $sum$ is updated. Especially with increasing sizes of $N$, the overhead of parallelization will be less. We can use a reduction clause here to divide the work equally. $sum$ will is divided into four local copies and added together after the loop.
	
	To count the number of flops, we need to know how many floating point operations are done in the for loop for every iteration of $N$. This is the total number of floating point operations. Divided by the time it takes to run this, gives the operations per second for the whole system. If multiple threads are used, we need to account for the number of threads if we want the flops performance per thread(assuming each threads has the same performance, which often is not the case).
	
	The number of floating operation inside the for-loop is 10, when counting multiplication, addition, dividing and casting as one flop and assuming no compiler optimizations. 
	
	\item The program is run with different sizes of $N$ and different amounts of threads, see \autoref{tbl:pi} and \autoref{tbl:piflops}.
	It is run on a two-core system of 3,4Ghz.

\end{enumerate}
\newpage
	
	\begin{table}
		\centering
		\caption{Time needed to approximate pi with different $N$.}
		\label{tbl:pi}
		\begin{tabular}{r|r|r|r|r|r}
			& \multicolumn{4}{c}{Time (s)} \\
			$N$ & 1 & 100 & 10000 & 100000000 \\ \hline
			Thread count \\ \cline{2-5}
			 1 & 0.00000 & 0.00000 & 0.00060 &  1.43500\\
			 2 & 0.00000 & 0.00000 & 0.00030 & 0.78000 \\
			 4 & 0.00000 & 0.00000 & 0.00030 & 0.75500 \\
			 8 & 0.00000 & 0.00000 & 0.00060 & 0.75500 \\
			Pi value & 3.2000 & 3.1600 & 3.1459 & 3.1459\\
		\end{tabular}
	\end{table} 
	 
	\begin{table}
		\centering
		\caption{Performance reached by system with different $N$.}
		\label{tbl:piflops}
		\begin{tabular}{r|r|r|r|r|r}
			& \multicolumn{4}{c}{MFlops } \\
			$N$ & 1 & 100 & 10000 & 100000000 \\ \hline
			Thread count \\ \cline{2-5}
		 1 &  &  & 150 & 700  \\
		 2 &  &  & 300 & 1270 \\
		 4 &  &  & 300 & 1310 \\
		 8 &  &  & 150 & 1310 \\
		\end{tabular}
	\end{table} 
		
\begin{enumerate}
\item As can be seen in the tables, the system scales well from 1 to 2 threads when using a high number of iterations $N$ does not increase the performance very well. Only with the highest number of iterations are 4 and 8 threads a little bit faster. In a system with two cores, the twofold performance increase from 1 to 2 threads is expected. 4 and 8 threads do increase the performance a bit probably due to a more efficient use of the available cores, but this maximum is already reached with 4 threads.

For low(er) number of iteration the calculation is very fast and each time it differs quite a lot, depending on other processes running and interfering probably. The amount of FLOPS shows the same pattern: two-fold performance increase from 1 to 2 threads and only slightly with more threads.

With all the results, it must be noted that performance scaling, and absolute performance may be quite different per system or architecture. Also compiler optimizations are very important,which might differ per system, language and compiler. 
\end{enumerate} 

\section{LUP decomposition}
\begin{enumerate}[(a)]
	\item \autoref{tbl:lup} shows a comparison of run times for the LUP decomposition and solving for $Ax=b$ for various matrix sizes. The first row (sequential) is the original code that has been edited to be able to generate matrices to allow for different matrix sizes. The other rows refer to the parallel code running with different numbers of threads. For this question we can compare the first two rows.
	
	As we can see from these rows it shows that the sequential code is a bit faster than the parallel code running with a single thread for a $1000 \times 1000$ matrix. This is caused by the parallel code having extra overhead to create and manage additional threads even though there is only one. This causes enough of a slow down to cause the program to run for almost one additional second.
	
	The run time for the $5000 \times 5000$ problem was so long for both the sequential and parallel code that we quit the program after running for 5 minutes.
	
	\item This is where the meat of the parallelisation comes from. When we look at \autopageref{tbl:lup} again we can see that the run time of the program decreases drastically when we increase the number of threads that the program uses. When the thread count is doubled the run time is almost cut in half so it seems that the problem does parallelize quite well.
	
	There is a change in the trend when we use 12 threads though. The run time increases by a lot for most of the problem sizes to the point where it takes longer to run than with a single thread. This is not true for a problem size of $5000 \times 5000$. The nodes in the cluster support just 12 threads so the slow down might be caused by the CPU being stressed so much that there is not enough time to communicate between threads or threads having to wait for each other for some other reason. Some parts of the parallel algorithm do not take a very long time to run in the first place and having so many threads might add additional overhead.

	\item To invert the matrix we can change the problem from $Ax=b$ to $AX=I$ where $I$ is the identity matrix. We can do this with the infrastructure that is already in place where we solve $Ax_i = I_i$ for each column $i$ in $X$ and $I$. This means that to invert a $n \times n$ matrix we need to do a single LUP decomposition and then solve using the decomposition $n$ times, $X$ will then hold the inverse of $A$.
	
	Run times for the code are shown in \autoref{tbl:invert}, the run time also includes the parallel multiplication routine. We also included using 10 threads to investigate why 12 threads has longer run times than lower thread counts. We removed the $5000 \times 5000$ problem because the time to complete that problem took longer than 5 minutes no matter the amount of threads used.
	
	We can see that the trend in \autoref{tbl:invert} is the same as in \autoref{tbl:lup}, when the number of threads is doubled then the run time is nearly cut in half. The only exception to this are 10 and 12 threads. Both of these show a slowdown for $n=10, n=100$, this slowdown is most likely caused by the overhead of creating and managing the respective number of threads while the amount of work doesn't justify that amount of threads. This means that more time is spend on managing the work instead of on doing the actual work. With 10 threads the amount of work to do for $n=1000$ is still sufficient to have a speed up but with 12 threads there is still too much overhead to see a speed up.
	
	\item We also included the run times for this routine in \autoref{tbl:invert}. The results of the routine aren't always very accurate, we do not obtain a proper identity matrix. Instead we obtain something which is almost an identity matrix, its main diagonal contains only ones or numbers that differ from 1 in the 6th decimal place. Other places in the matrix contain 0 or numbers that are very close to 0. This is most likely caused by rounding errors in floating point numbers.
\end{enumerate}

\begin{table}
	\centering
	\caption{Time needed for the LUP decomposition, empty fields are aborted runs because they took longer than 5 minutes.}
	\label{tbl:lup}
	\begin{tabular}{r|r|r|r|r}
		& \multicolumn{4}{c}{Time (s)} \\
		$n$ & 10 & 100 & 1000 & 5000 \\ \hline
		Thread count \\ \cline{2-5}
		Sequential & 0.00  & 0.01 & 3.57 & \\
		 1 & 0.00 & 0.01 & 4.48 &  \\
		 2 & 0.00 & 0.01 & 2.32 & 4:57.29 \\
		 4 & 0.00 & 0.01 & 1.25 & 2:28.00 \\
		 8 & 0.00 & 0.01 & 0.66 & 1:13.56 \\
		12 & 0.05 & 0.46 & 6.49 & 49.23 \\
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Time needed to invert a matrix}
	\label{tbl:invert}
	\begin{tabular}{r|r|r|r}
		& \multicolumn{3}{c}{Time (s)} \\
		$n$ & 10 & 100 & 1000 \\ \hline
		Thread count \\ \cline{2-4}
		 1 & 0.00 & 0.06 & 30.02 \\
		 2 & 0.00 & 0.04 & 15.57 \\
		 4 & 0.00 & 0.04 & 7.95 \\
		 8 & 0.01 & 0.03 & 4.16 \\
		10 & 0.03 & 0.06 & 3.61 \\
		12 & 0.04 & 0.03 & 6.14 \\
	\end{tabular}
\end{table}

\section{Wave equation}
\begin{enumerate}[(a)]
\item When the program is compiled and run, it creates individual pictures, frames, of a simulation of waves created by different points of origin. Several parameters are configurable, like the number of wave creation points, grid size, number of frames and time lapse per frame. With the included script and the program mplayer, the frames are assembled into a movie to visualize the simulation.

The program further measures the time need to compute each frame, so the effect of different parameters can be observed as well as a parallel implementation.

\item Almost all for-loops can be parallelized, but some for-loops are not very big, so the effect would be minimal or even negative on the processing efficiency due to overhead for thread creation and handling.

The following fragments from the original code are parallelized, from top to bottom:
\lstinputlisting[firstline=57,lastline=63]{wavePar.c}

In the function initialize(),this loop is parallelized simply by dividing the loop in equal chunks (with static), as indicated by the set number of threads(system level). Although the number of wave sources (nsrc) is not very large generally, the workload in the loop is large enough to gain a performance increase.

\lstinputlisting[firstline=69,lastline=77]{wavePar.c}

This is a large loop, defined by the number of frames and the grid size per frame, so the performance gain is large in this one. Variable i is declared private explicitly, because only k is private automatically by the $omp$ directive. Divided in equal chunks.

\lstinputlisting[firstline=81,lastline=96]{wavePar.c}

These initialization steps can be parallelized efficiently, each thread doing an equal sized chunk, different part of the matrix u. Variable j is declared private because of the inner loop.

\lstinputlisting[firstline=103,lastline=118]{wavePar.c}

In the function boundary() ,the first, nested, loop can also be nicely parallized, for equal parts of the matrix $u$ for each thread. The inner loop variable $j$ is declared private, since this is not automatic.

The second loop has an efficiency gain, although $nrsc$ can be small, since the body is enough work to take advantage of the multiple threads.
\lstinputlisting[firstline=129,lastline=144]{wavePar.c}

The first for loop can parallelized and would divide the computation of each frame between each thread. However, because the function boundary() is already greatly parallelized, we chosen for multi threading the computations within each frame. Variable $j$ is made private. Although the update operation is an incremental one, each iteration increments an unique element of $u$, so no precaution have to be taken here.
\lstinputlisting[firstline=157,lastline=175]{wavePar.c}

In the function stretchContrast(), three for-loops are nested to determine the contrast range is each frame. Since min and max are shared, and read and written both, we give each thread its own copy with the reduction clause. We can use the operators min and max to compare the results of each thread the same way as the evaluation in the inner loop, to update the variables min and max with the results of each thread.
\lstinputlisting[firstline=180,lastline=190]{wavePar.c}

Also in the function stretchContrast(), the scaling for each frame can be parallelized, declaring the inner loop variables private.
\lstinputlisting[firstline=201,lastline=253]{wavePar.c}

In the function saveFrames(), all for-loops can be parallelized simply in equal chunks and declaring the inner loop variables private.

In the functions parseRealOpt() and parseIntOpt(), there are two for-loops. These loops are very small, evaluating only the input parameters for the program, if any are specified when starting the program. There we have chosen not to parallelize the, since the gain would be minimal.

\item 
To measure the speed of the parallelization, two different settings are used and 4 different number of threads. The measurement is performed on a 4 core machine with no hyperthreading(AMD Phenom II processor)

With standard settings where number of wave sources is 10, number of frames 100, grid size 300, colour, 0,1 seconds time spacing and 0,5 is the velocity of waves, see \autoref{tbl:waves1}:

As can be seen in \autoref{tbl:waves1}, the calculation speeds up with increasing number of threads, except for the 8 threads setting. this is due to the machine being a 4 core machine. Also the speed up is not completely linear, due to the program not being completely parallel, so there is some waiting time for 1 thread to finish.


Using setting where the wave sources are 50, grid size 1200 and the number of frames 200 we get the following calculation times in \autoref{tbl:waves2}
Also with the more demanding settings, the speedup is significant, but not completely linear. The 8 thread setting takes a bit longer than the 4 thread setting. With these settings and 8 threads the overhead and waiting times for extra threads are somewhat increased, due to the 4 core limitation.
\end{enumerate}

\begin{table}[h!]
	\centering
	\caption{Time needed to calculate the wave frames}
	\label{tbl:waves1}
	\begin{tabular}{r|r|r}
		& Calculation time(s) \\
		Thread count \\ \cline{2-3}
		Serial(1) & 0.45  \\
		2 & 0.25  \\
		4 & 0.15  \\
		8 & 0.15  \\
		
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Time needed to calculate the wave frames, high setting}
	\label{tbl:waves2}
	\begin{tabular}{r|r|r}
		& Calculation time(s) \\
		Thread count \\ \cline{2-3}
		Serial(1) & 7.10  \\
		2 & 3.60  \\
		4 & 2.00 \\
		8 & 2.10  \\
		
	\end{tabular}
\end{table}

\newpage
\appendix
\section {Program code of $\pi$}
\lstinputlisting[caption={pi.c},label={app:piPar}]{pi.c}

\section{Program code of LUP decomposition}
\lstinputlisting[caption={lup.c}]{lup.c}

\section {Program code of wave equation}
\label{app:wavePar}
\lstinputlisting[caption={wave.c}]{wavePar.c}

\end{document}