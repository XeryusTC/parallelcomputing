\documentclass[a4paper]{article}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{listings}

\title{Assignment 1}
\author{Jacco Spoelder (s1348493) \and Xeryus Stokkel (s2332795)}

\begin{document}

\maketitle

\section{Exercise 1}
\begin{enumerate}[(a)]
	\item The "Hello world!" lines are interleaved because the statements in the first block are executed in parallel. This means that both threads will simultaneously execute the \texttt{printf("Hello ");} statement so you see that appearing twice. They will then execute the \texttt{printf("world!\textbackslash n");} statements in parallel which means that this is printed twice in a row. Because it happens in parallel the lines will be interleaved. We were not able to reproduce this interleaved effect on our own systems.
	
	The line "Have a nice day!" is not interleaved because only a single printf statement is executed in parallel, this simply comes down to executing this statement twice.
	\item The output is the following:
		\begin{lstlisting}
Hello world!
Have a nice day!
Hello world!
Have a nice day!
Have fun!
		\end{lstlisting}
		As we can see, everything in the first parallel block is executed twice. However the second pragma statement has no effect, the line "Have a nice day!" is only printed twice just like the other printf statement in the first block. This means that OpenMP doesn't support nesting or that it needs to be enabled explicitly.
	\item The output when the environment variable \texttt{OMP\_NESTING} is enabled is:
		\begin{lstlisting}
Hello world!
Hello world!
Have a nice day!
Have a nice day!
Have a nice day!
Have a nice day!
Have fun!
		\end{lstlisting}
		As we can see the line "Have a nice day!" is printed four times. We have 2 threads executing the main parallel block, so that means printing the line twice. But we also have a nested pragma that wants to execute the print statement in parallel. This means that the statement is executed an additional 2 times per already existing thread. So in total we get $2 \times 2 = 4$ total outputs. In general we can say that with N threads that the entire block will be executed N times and the nested pragma will be executed N times for each already existing block, so in general the amount of printed "Have a nice day!" statements will be $N \times N = N^2$.
\end{enumerate}

\section{Exercise 2}
\begin{enumerate}[(a)]
	\item We predict that the output of the program will look like below. \texttt{undefined} means that we do not know the output as the variable is not initialised before printing.
		\begin{lstlisting}
#i=31415
i=undefined
i=undefined
i=undefined
i=undefined
#i=31415
i=31415
i=31415
i=31415
i=31415
#i=31415
i=31415
i=31416
i=31417
i=31418
#i=31419
		\end{lstlisting}
		The compiler warns us about using \texttt{i} uninitialised in the first parallel block. The output was:
		\begin{lstlisting}
#i=31415
 i=0
 i=32734
 i=0
 i=0
#i=31415
 i=31415
 i=31415
 i=31415
 i=31415
#i=31415
 i=31415
 i=31417
 i=31416
 i=31418
#i=31419
		\end{lstlisting}
		As we can see our prediction was correct, the value of the first parallel section was indeed undefined: 0 and 32734 both occur without an indication of why because \texttt{i} was undefined. We also see that when the variable is shared that the value is affected across threads and that it also affects the value of the variable in the main thread.
	\setcounter{enumi}{0}
	\item When executing sequential the program will simply do the following $N$ times:
		\begin{enumerate}[1.]
			\item Set the value of $x$ to 1.
			\item Increment $x$ to 2.
			\item Multiply $x$ by 3 to obtain 6.
			\item Increment the element with index 6 in the histogram by one.
		\end{enumerate}
		This means that the resulting histogram will be \texttt{\{0 0 0 0 0 0 1000000 0 0 0\}}.
		
		After executing the program this has been shown to indeed be the case.
	\item The possibilities of what happens with 2 threads are the following:
		\begin{itemize}
			\item The same as with one thread.
			\item The same as with one thread but step 2 and 3 reversed, so $x = 1 \cdot 3 + 1 = 4$.
			\item Step 2 is skipped so the result is $x = 3$.
			\item Step 3 is skipped so the result is $x = 2$.
		\end{itemize}
		So the possibilities for the value of $x$ are 2, 3, 4 and 6.
	\item With \texttt{OMP\_NUM\_THREADS} = 2 the output is not consistent but the order of magnitudes of the numbers is always the same:
	\begin{lstlisting}
0: 0
1: 0
2: 62
3: 1363
4: 16
5: 0
6: 998559
7: 0
8: 0
9: 0
	\end{lstlisting}
	As we can see our prediction of the values of $x$ was correct. We can also see that most of the time the code executes correctly. The largest error occurs where the second section overwrites the first section. This is most likely because the calculations are done on a temporary variable $y$. Both sections will retrieve the value of $x$ and store them in $y$ and then do their calculations on this variable. Section one will then write the value of $y$ to $x$. Section 2 will then also do the same, but because it used the original value of $x$ it will overwrite the results of the first section.
	
	Similar events occur for the case where $x = 2$. For the case where $x=4$ the sections do not overwrite each other but section 2 finishes before section 1 starts and thus the calculation is done the wrong way around.
	
	For 24 threads the following happens (note that we ran this on a laptop with only 4 cores available):
	\begin{lstlisting}
0: 0
1: 0
2: 186
3: 565
4: 183
5: 0
6: 999066
7: 0
8: 0
9: 0
	\end{lstlisting}
	The results are very similar. We think this is because only two threads actually do any work and 22 are sitting around and idling. This means that the results are the same for 2 threads. It is very likely that the program takes a lot longer because of the overhead of creating and destroying 24 threads for every iteration of the for loop.
	\item The results for this program are shown in \autoref{tbl:shared}. It clearly shows that the execution time for the reductive part goes down when the number of threads goes up. For the shared part the execution time increases quite dramatically!
	
	Reduction executes the code in parallel and adds the results together after all the threads finish. The shared variable code updates the results in every iteration, so all the threads compete for the same variable which results in large waiting times. Reduction can increase its speed because there is only a slowdown at the end where all the results are added together. The overhead of adding the variables together is also only minimal, with 8 threads only 8 sums need to be added together so the overhead is minimal compared to total number of calculations (100 million).
	
	Shared code needs to wait these 100 million calculations to update. Whenever a thread updates the other threads need to wait for this process to be completed. This means that with 4 threads that there can be a queue of up to 3 threads idling because they need to wait for the shared variable to be released. With 8 threads the queue increases to a maximum of 7, this means that a lot more idling is done and no calculations are done in that time. So the program is spending most of the time waiting to update instead of doing the calculations. This is especially noticeable with this program where the calculation is really simple so updating the value in memory takes a relatively long time.
	
	With the step from 1 to 2 threads we can see that the calculation time quadruples while for the other steps it only doubles. From this we can say that the calculation takes less time than updating the shared variable because with 1 thread there is no wait time while with 2 threads there is a lot of wait time. This means that each thread spends most of its time waiting on the other thread instead of doing the calculation. With 4 and 8 threads the time only doubles so we can say that the wait time also doubles. This means that the queue to update the shared variable is always full because a thread will do the calculation before any other thread has been able to update the shared variable.
\end{enumerate}

\begin{table}
	\centering
	\caption{Timings of the reduction and the shared part of the program}
	\label{tbl:shared}
	\begin{tabular}{r|l|l}
		Threads & Reduction & Shared \\ \hline
		1 & 0.389616 & 1.895604 \\
		2 & 0.195814 & 16.035956 \\
		4 & 0.149522 & 35.542965 \\
		8 & 0.102292 & 72.173635
	\end{tabular}
\end{table}

\section{Computing $\pi$}

\section{LUP decomposition}
\begin{enumerate}[(a)]
	\item \autoref{tbl:lup} shows a comparison of run times for the LUP decomposition and solving for $Ax=b$ for various matrix sizes. The first row (sequential) is the original code that has been edited to be able to generate matrices to allow for different matrix sizes. The other rows refer to the parallel code running with different numbers of threads. For this question we can compare the first two rows.
	
	As we can see from these rows it shows that the sequential code is a bit faster than the parallel code running with a single thread for a $1000 \times 1000$ matrix. This is caused by the parallel code having extra overhead to create and manage additional threads even though there is only one. This causes enough of a slow down to cause the program to run for almost one additional second.
	
	The run time for the $5000 \times 5000$ problem was so long for both the sequential and parallel code that we quit the program after running for 5 minutes.
	
	\item This is where the meat of the parallelisation comes from. When we look at \autopageref{tbl:lup} again we can see that the run time of the program decreases drastically when we increase the number of threads that the program uses. When the thread count is doubled the run time is almost cut in half so it seems that the problem does parallelize quite well.
	
	There is a change in the trend when we use 12 threads though. The run time increases by a lot for most of the problem sizes to the point where it takes longer to run than with a single thread. This is not true for a problem size of $5000 \times 5000$. The nodes in the cluster support just 12 threads so the slow down might be caused by the CPU being stressed so much that there is not enough time to communicate between threads or threads having to wait for each other for some other reason. Some parts of the parallel algorithm do not take a very long time to run in the first place and having so many threads might add additional overhead.

	\item To invert the matrix we can change the problem from $Ax=b$ to $AX=I$ where $I$ is the identity matrix. We can do this with the infrastructure that is already in place where we solve $Ax_i = I_i$ for each column $i$ in $X$ and $I$. This means that to invert a $n \times n$ matrix we need to do a single LUP decomposition and then solve using the decomposition $n$ times, $X$ will then hold the inverse of $A$.
	
	Run times for the code are shown in \autoref{tbl:invert}, the run time also includes the parallel multiplication routine. We also included using 10 threads to investigate why 12 threads has longer run times than lower thread counts. We removed the $5000 \times 5000$ problem because the time to complete that problem took longer than 5 minutes no matter the amount of threads used.
	
	We can see that the trend in \autoref{tbl:invert} is the same as in \autoref{tbl:lup}, when the number of threads is doubled then the run time is nearly cut in half. The only exception to this are 10 and 12 threads. Both of these show a slowdown for $n=10, n=100$, this slowdown is most likely caused by the overhead of creating and managing the respective number of threads while the amount of work doesn't justify that amount of threads. This means that more time is spend on managing the work instead of on doing the actual work. With 10 threads the amount of work to do for $n=1000$ is still sufficient to have a speed up but with 12 threads there is still too much overhead to see a speed up.
	
	\item We also included the run times for this routine in \autoref{tbl:invert}. The results of the routine aren't always very accurate, we do not obtain a proper identity matrix. Instead we obtain something which is almost an identity matrix, its main diagonal contains only ones or numbers that differ from 1 in the 6th decimal place. Other places in the matrix contain 0 or numbers that are very close to 0. This is most likely caused by rounding errors in floating point numbers.
\end{enumerate}

\begin{table}
	\centering
	\caption{Time needed for the LUP decomposition, empty fields are aborted runs because they took longer than 5 minutes.}
	\label{tbl:lup}
	\begin{tabular}{r|r|r|r|r}
		& \multicolumn{4}{c}{Time (s)} \\
		$n$ & 10 & 100 & 1000 & 5000 \\ \hline
		Thread count \\ \cline{2-5}
		Sequential & 0.00  & 0.01 & 3.57 & \\
		 1 & 0.00 & 0.01 & 4.48 &  \\
		 2 & 0.00 & 0.01 & 2.32 & 4:57.29 \\
		 4 & 0.00 & 0.01 & 1.25 & 2:28.00 \\
		 8 & 0.00 & 0.01 & 0.66 & 1:13.56 \\
		12 & 0.05 & 0.46 & 6.49 & 49.23 \\
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Time needed to invert a matrix}
	\label{tbl:invert}
	\begin{tabular}{r|r|r|r}
		& \multicolumn{3}{c}{Time (s)} \\
		$n$ & 10 & 100 & 1000 \\ \hline
		Thread count \\ \cline{2-4}
		 1 & 0.00 & 0.06 & 30.02 \\
		 2 & 0.00 & 0.04 & 15.57 \\
		 4 & 0.00 & 0.04 & 7.95 \\
		 8 & 0.01 & 0.03 & 4.16 \\
		10 & 0.03 & 0.06 & 3.61 \\
		12 & 0.04 & 0.03 & 6.14 \\
	\end{tabular}
\end{table}

\end{document}